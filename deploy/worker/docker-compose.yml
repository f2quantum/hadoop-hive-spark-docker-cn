services:
  worker1:
    image: hadoop-hive-spark-worker
    hostname: worker1
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.3
      SPARK_LOCAL_HOSTNAME: worker1
    volumes:
      - datanode1:/opt/hadoop/dfs/data
    restart: always
    network_mode: "host"
    extra_hosts:
      - "metastore:172.28.1.1"
      - "master:master.net"

volumes:
  namenode:
  namesecondary:
  datanode1:
  datanode2:
  metastore: