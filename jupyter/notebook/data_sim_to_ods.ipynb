{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODS init spark content successful\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data-Importer\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set hive.exec.dynamici.partition=true;\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode=nonstrict;\")\n",
    "\n",
    "# 源数据 => ODS\n",
    "print(\"ODS init spark content successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init ods ods_time_series ,ods_job  Success\n"
     ]
    }
   ],
   "source": [
    "# create database time_series\n",
    "spark.sql(\"DROP TABLE IF EXISTS ods_time_series;\")\n",
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ods_time_series\n",
    "(id bigint,\n",
    "time TIMESTAMP,\n",
    "lon float,\n",
    "lat float)\n",
    "PARTITIONED BY(yearMonth string)\n",
    "STORED AS PARQUET;\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "\n",
    "# create database job\n",
    "spark.sql(\"DROP TABLE IF EXISTS ods_job;\")\n",
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ods_job\n",
    "(id bigint,start_time TIMESTAMP,end_time TIMESTAMP)\n",
    "PARTITIONED BY(yearMonth string)\n",
    "STORED AS PARQUET;\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "\n",
    "# 创建输出表\n",
    "#CREATE TABLE output(id SERIAL PRIMARY KEY,job_id int4);\n",
    "\n",
    "print(\"Init ods ods_time_series ,ods_job  Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data local inpath './data/timeseries.csv' into table ods_time_series_external ;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入临时表 ods_time_series_external\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS ods_time_series_external;\")\n",
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ods_time_series_external(\n",
    " id string,\n",
    " time string,\n",
    " lon string,\n",
    " lat string)\n",
    " ROW FORMAT DELIMITED\n",
    " FIELDS TERMINATED BY ',';\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "    \n",
    "sql = f\"load data local inpath './data/timeseries.csv' into table ods_time_series_external ;\"\n",
    "print(sql)\n",
    "spark.sql(sql)\n",
    "\n",
    "# 临时表写入ODS表\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSERT INTO ods_time_series partition(yearMonth)\n",
      "SELECT\n",
      " cast(id as bigint) as id,\n",
      " cast(cast(time  as int) as timestamp) as time,\n",
      " cast(lon as float) as lon,\n",
      " cast(lat as float) as lat,\n",
      " date_format(cast(cast(time  as int) as timestamp),'yyyyMM') as yearMonth\n",
      " FROM ods_time_series_external;\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+-------------------+--------+--------+---------+\n",
      "|      id|               time|     lon|     lat|yearMonth|\n",
      "+--------+-------------------+--------+--------+---------+\n",
      "|14711039|2026-05-01 00:00:00|45.23426|46.23426|   202605|\n",
      "+--------+-------------------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 临时表写入ODS表\n",
    "\n",
    "# see https://stackoverflow.com/questions/62607279/how-to-load-a-csv-file-containing-time-string-value-to-timestamp-in-hive\n",
    "\n",
    "sql = '''\n",
    "INSERT INTO ods_time_series partition(yearMonth)\n",
    "SELECT\n",
    " cast(id as bigint) as id,\n",
    " cast(cast(time  as int) as timestamp) as time,\n",
    " cast(lon as float) as lon,\n",
    " cast(lat as float) as lat,\n",
    " date_format(cast(cast(time  as int) as timestamp),'yyyyMM') as yearMonth\n",
    " FROM ods_time_series_external;\n",
    "'''\n",
    "print(sql)\n",
    "result = spark.sql(sql)\n",
    "result.show()\n",
    "\n",
    "# 查看一条数据\n",
    "result = spark.sql(\"SELECT * FROM ods_time_series LIMIT 1\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|31535907|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看数据规模\n",
    "result = spark.sql(\"SELECT count(*) FROM ods_time_series \")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data local inpath './data/jobs.csv' into table ods_job_external ;\n",
      "+---+----------+----------+\n",
      "| id|start_time|  end_time|\n",
      "+---+----------+----------+\n",
      "|  0|1704038400|1704042000|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读入临时表 ods_time_series_external\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS ods_job_external;\")\n",
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ods_job_external(\n",
    " id string,\n",
    " start_time string,\n",
    " end_time string)\n",
    " ROW FORMAT DELIMITED\n",
    " FIELDS TERMINATED BY ',';\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "    \n",
    "sql = f\"load data local inpath './data/jobs.csv' into table ods_job_external ;\"\n",
    "print(sql)\n",
    "spark.sql(sql)\n",
    "\n",
    "\n",
    "# 查看一条数据\n",
    "result = spark.sql(\"SELECT * FROM ods_job_external LIMIT 1\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSERT INTO ods_job partition(yearMonth)\n",
      "SELECT\n",
      " cast(id as bigint) as id,\n",
      " cast(cast(start_time as int) as timestamp) as start_time,\n",
      " cast(cast(end_time as int) as timestamp) as end_time,\n",
      " date_format(cast(cast(start_time  as int) as timestamp),'yyyyMM') as yearMonth\n",
      " FROM ods_job_external;\n",
      "\n",
      "+-----+-------------------+-------------------+---------+\n",
      "|   id|         start_time|           end_time|yearMonth|\n",
      "+-----+-------------------+-------------------+---------+\n",
      "|30656|2027-07-01 00:00:00|2027-07-01 01:00:00|   202707|\n",
      "+-----+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 临时表写入ODS表 ods_job\n",
    "\n",
    "# see https://stackoverflow.com/questions/62607279/how-to-load-a-csv-file-containing-time-string-value-to-timestamp-in-hive\n",
    "\n",
    "sql = '''\n",
    "INSERT INTO ods_job partition(yearMonth)\n",
    "SELECT\n",
    " cast(id as bigint) as id,\n",
    " cast(cast(start_time as int) as timestamp) as start_time,\n",
    " cast(cast(end_time as int) as timestamp) as end_time,\n",
    " date_format(cast(cast(start_time  as int) as timestamp),'yyyyMM') as yearMonth\n",
    " FROM ods_job_external;\n",
    "'''\n",
    "print(sql)\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# 查看一条数据\n",
    "result = spark.sql(\"SELECT * FROM ods_job LIMIT 1\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|date_format(time, yyyyMM)|\n",
      "+-------------------------+\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "|                   202503|\n",
      "+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看分区数目\n",
    "result = spark.sql(\"select date_format(time,'yyyyMM') from ods_time_series ;\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
