{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 集群名称:  dwd_to_dwm\n",
      "Spark 集群版本:  3.3.3\n",
      "Spark ID:  application_1710158715244_0002\n",
      "Spark 集群节点数:  Set(worker1:44749, jupyter:36873, worker2:45849)\n",
      "每个 Executor 的内存容量:  40g\n"
     ]
    }
   ],
   "source": [
    "# 基于空间索引 给时序数据area打标\n",
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark.sql import SparkSession\n",
    "# https://sedona.apache.org/1.3.0-incubating/archive/tutorial/geospark-sql-python/\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "app_name = \"dwd_to_dwm\"\n",
    "\n",
    "# GeoSpark has a suite of well-written geometry and index serializers. \n",
    "# Forgetting to enable these serializers will lead to high memory consumption. (序列化器优化内存使用)\n",
    "# https://stackoverflow.com/questions/65213369/unable-to-configure-geospark-in-spark-session\n",
    "upload_jars()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.executor.memory\", \"40g\") \\\n",
    "    .config(\"spark.driver.memory\", \"40g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# GeoSparkRegistrator.registerAll(spark)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 打印集群信息\n",
    "print(\"Spark 集群名称: \", spark.conf.get(\"spark.app.name\"))\n",
    "print(\"Spark 集群版本: \", spark.version)\n",
    "print(\"Spark ID: \", spark.sparkContext.applicationId)\n",
    "\n",
    "print(\"Spark 集群节点数: \", spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet())\n",
    "print(\"每个 Executor 的内存容量: \", spark.conf.get(\"spark.executor.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化 dwm_time_series，dwm_job成功\n"
     ]
    }
   ],
   "source": [
    "# create database time_series\n",
    "spark.sql(\"DROP TABLE IF EXISTS dwm_time_series;\")\n",
    "\n",
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS dwm_time_series\n",
    "         (id bigint,\n",
    "          time TIMESTAMP,\n",
    "          lon float,\n",
    "          lat float)\n",
    "PARTITIONED BY(area int)\n",
    "STORED AS PARQUET;\n",
    "'''\n",
    "\n",
    "# create database job\n",
    "spark.sql(\"DROP TABLE IF EXISTS dwm_job;\")\n",
    "sql = \"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dwm_job\n",
    "(id bigint,start_time TIMESTAMP,end_time TIMESTAMP)\n",
    "PARTITIONED BY(yearMonth string)\n",
    "STORED AS PARQUET;\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "\n",
    "# 创建输出表\n",
    "#CREATE TABLE output(id SERIAL PRIMARY KEY,job_id int4);\n",
    "\n",
    "print(\"初始化 dwm_time_series，dwm_job成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+---------+\n",
      "|   id|         start_time|           end_time|yearMonth|\n",
      "+-----+-------------------+-------------------+---------+\n",
      "|13136|2025-07-01 00:00:00|2025-07-01 01:00:00|   202507|\n",
      "+-----+-------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载job 表=> \n",
    "\n",
    "sql = '''\n",
    "INSERT INTO dwm_job partition(yearMonth)\n",
    "SELECT\n",
    " id as id,\n",
    " start_time as start_time,\n",
    " end_time as end_time,\n",
    " yearMonth as yearMonth\n",
    " FROM dwd_job;\n",
    "'''\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# 查看一条数据\n",
    "result = spark.sql(\"SELECT * FROM dwm_job LIMIT 1\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>         (26 + 2) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|315359976|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 查看时序数据总量\n",
    "result = spark.sql(\"SELECT count(*) FROM dwd_time_series\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (3 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+---------+\n",
      "| id|               time|      lon|      lat|\n",
      "+---+-------------------+---------+---------+\n",
      "|719|2023-12-31 17:00:00| 45.00023| 46.00023|\n",
      "|720|2023-12-31 17:00:05| 45.00023| 46.00023|\n",
      "|721|2023-12-31 17:00:10| 45.00023| 46.00023|\n",
      "|722|2023-12-31 17:00:15| 45.00023| 46.00023|\n",
      "|723|2023-12-31 17:00:20| 45.00023| 46.00023|\n",
      "|724|2023-12-31 17:00:25| 45.00023| 46.00023|\n",
      "|725|2023-12-31 17:00:30| 45.00023| 46.00023|\n",
      "|726|2023-12-31 17:00:35| 45.00023| 46.00023|\n",
      "|727|2023-12-31 17:00:40| 45.00023| 46.00023|\n",
      "|728|2023-12-31 17:00:45|45.000233|46.000233|\n",
      "|729|2023-12-31 17:00:50|45.000233|46.000233|\n",
      "|730|2023-12-31 17:00:55|45.000233|46.000233|\n",
      "|731|2023-12-31 17:01:00|45.000233|46.000233|\n",
      "|732|2023-12-31 17:01:05|45.000233|46.000233|\n",
      "|733|2023-12-31 17:01:10|45.000233|46.000233|\n",
      "|734|2023-12-31 17:01:15|45.000233|46.000233|\n",
      "|735|2023-12-31 17:01:20|45.000233|46.000233|\n",
      "|736|2023-12-31 17:01:25|45.000233|46.000233|\n",
      "|737|2023-12-31 17:01:30|45.000233|46.000233|\n",
      "|738|2023-12-31 17:01:35|45.000233|46.000233|\n",
      "+---+-------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:======================================>                   (4 + 2) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT id,time,lon,lat\n",
    "FROM (\n",
    "\tSELECT id,time,lon, lat\n",
    "\tFROM dwd_time_series as time_series\n",
    "\tWHERE EXISTS (\n",
    "\t\tSELECT *\n",
    "\t\tFROM dwm_job as job\n",
    "\t\tWHERE job.id = 1\n",
    "\t\t\tAND time_series.time BETWEEN job.start_time AND job.end_time\n",
    "\t)\n",
    ") dwd_time_series;\n",
    "\n",
    "'''\n",
    "df_input = spark.sql(sql)\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', LongType(), False), StructField('time', TimestampType(), False), StructField('lon', FloatType(), False), StructField('lat', FloatType(), False), StructField('area', IntegerType(), False)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, FloatType, IntegerType\n",
    "\n",
    "# 定义模式（schema）\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"time\", TimestampType(), nullable=False),\n",
    "    StructField(\"lon\", FloatType(), nullable=False),\n",
    "    StructField(\"lat\", FloatType(), nullable=False)\n",
    "])\n",
    "\n",
    "# 添加分区列\n",
    "schema.add(StructField(\"area\", IntegerType(), nullable=False))\n",
    "\n",
    "# 打印模式（schema）\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "import rtree\n",
    "# 创建R树索引\n",
    "idx = rtree.index.Index()\n",
    "\n",
    "def build_rtree_idx():\n",
    "    id_list =  df_input.select(\"id\").rdd.flatMap(lambda x: x).collect()\n",
    "    lon_list = df_input.select(\"lon\").rdd.flatMap(lambda x: x).collect()\n",
    "    lat_list = df_input.select(\"lat\").rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    for id,lon,lat in zip(id_list,lon_list,lat_list):\n",
    "        idx.insert(id,lon,lat,lon,lat)\n",
    "    \n",
    "def mapper(rows):\n",
    "    region_numbers={}\n",
    "    ResRow = Row()\n",
    "    # 针对一个part的数据做map\n",
    "    for row in rows:\n",
    "        area = 0\n",
    "        lon = row.lon\n",
    "        lat = row.lat\n",
    "        \n",
    "        nearest_points = list(idx.nearest(query_point, 1))  # 从R树索引中获取离查询点最近的1个点的ID\n",
    "        yield Row(id= row.id, time=row.time, lon=row.lon, lat=row.lat,area = area)\n",
    "        \n",
    "\n",
    "build_rtree_idx()\n",
    "\n",
    "\n",
    "#df_output = df_input.rdd.repartition(30).mapPartitions(mapper).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rtree.index.Index(bounds=[1.7976931348623157e+308, 1.7976931348623157e+308, -1.7976931348623157e+308, -1.7976931348623157e+308], size=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_point = (2, 2)  # 示例查询点的坐标\n",
    "\n",
    "# 使用R树索引查找最近点\n",
    "nearest_points = list(idx.nearest(query_point, 1))  # 从R树索引中获取离查询点最近的1个点的ID\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+------------------+----+\n",
      "|  id|               time|               lon|               lat|area|\n",
      "+----+-------------------+------------------+------------------+----+\n",
      "| 979|2023-12-31 17:21:40|45.000308990478516|46.000308990478516|   0|\n",
      "| 980|2023-12-31 17:21:45|45.000308990478516|46.000308990478516|   0|\n",
      "| 981|2023-12-31 17:21:50| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 982|2023-12-31 17:21:55| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 983|2023-12-31 17:22:00| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 984|2023-12-31 17:22:05| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 985|2023-12-31 17:22:10| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 986|2023-12-31 17:22:15| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 987|2023-12-31 17:22:20| 45.00031280517578| 46.00031280517578|   0|\n",
      "| 988|2023-12-31 17:22:25| 45.00031280517578| 46.00031280517578|   0|\n",
      "|1279|2023-12-31 17:46:40|45.000404357910156|46.000404357910156|   0|\n",
      "|1280|2023-12-31 17:46:45|45.000404357910156|46.000404357910156|   0|\n",
      "|1281|2023-12-31 17:46:50|45.000404357910156|46.000404357910156|   0|\n",
      "|1282|2023-12-31 17:46:55| 45.00040817260742| 46.00040817260742|   0|\n",
      "|1283|2023-12-31 17:47:00| 45.00040817260742| 46.00040817260742|   0|\n",
      "|1284|2023-12-31 17:47:05| 45.00040817260742| 46.00040817260742|   0|\n",
      "|1285|2023-12-31 17:47:10| 45.00040817260742| 46.00040817260742|   0|\n",
      "|1286|2023-12-31 17:47:15| 45.00040817260742| 46.00040817260742|   0|\n",
      "|1287|2023-12-31 17:47:20| 45.00040817260742| 46.00040817260742|   0|\n",
      "|1288|2023-12-31 17:47:25| 45.00040817260742| 46.00040817260742|   0|\n",
      "+----+-------------------+------------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
