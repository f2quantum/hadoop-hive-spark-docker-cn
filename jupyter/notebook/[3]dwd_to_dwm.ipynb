{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# 基于空间索引 给时序数据area打标\n",
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark.sql import SparkSession\n",
    "# https://sedona.apache.org/1.3.0-incubating/archive/tutorial/geospark-sql-python/\n",
    "from geospark.register import upload_jars\n",
    "from geospark.register import GeoSparkRegistrator\n",
    "app_name = \"dwd_to_dwm\"\n",
    "\n",
    "# GeoSpark has a suite of well-written geometry and index serializers. \n",
    "# Forgetting to enable these serializers will lead to high memory consumption. (序列化器优化内存使用)\n",
    "# https://stackoverflow.com/questions/65213369/unable-to-configure-geospark-in-spark-session\n",
    "# upload_jars()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.executor.memory\", \"40g\") \\\n",
    "    .config(\"spark.driver.memory\", \"40g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",\"4g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# GeoSparkRegistrator.registerAll(spark)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 打印集群信息\n",
    "print(\"Spark 集群名称: \", spark.conf.get(\"spark.app.name\"))\n",
    "print(\"Spark 集群版本: \", spark.version)\n",
    "print(\"Spark ID: \", spark.sparkContext.applicationId)\n",
    "\n",
    "print(\"Spark 集群节点数: \", spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keySet())\n",
    "print(\"每个 Executor 的内存容量: \", spark.conf.get(\"spark.executor.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database time_series\n",
    "spark.sql(\"DROP TABLE IF EXISTS dwm_time_series;\")\n",
    "\n",
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS dwm_time_series\n",
    "         (id bigint,\n",
    "          time TIMESTAMP,\n",
    "          lon float,\n",
    "          lat float)\n",
    "PARTITIONED BY(area int)\n",
    "STORED AS PARQUET;\n",
    "'''\n",
    "\n",
    "# create database job\n",
    "spark.sql(\"DROP TABLE IF EXISTS dwm_job;\")\n",
    "sql = \"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dwm_job\n",
    "(id bigint,start_time TIMESTAMP,end_time TIMESTAMP)\n",
    "PARTITIONED BY(yearMonth string)\n",
    "STORED AS PARQUET;\n",
    "\n",
    "\"\"\"\n",
    "spark.sql(sql)\n",
    "\n",
    "# 创建输出表\n",
    "#CREATE TABLE output(id SERIAL PRIMARY KEY,job_id int4);\n",
    "\n",
    "print(\"初始化 dwm_time_series，dwm_job成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载job 表=> \n",
    "\n",
    "sql = '''\n",
    "INSERT INTO dwm_job partition(yearMonth)\n",
    "SELECT\n",
    " id as id,\n",
    " start_time as start_time,\n",
    " end_time as end_time,\n",
    " yearMonth as yearMonth\n",
    " FROM dwd_job;\n",
    "'''\n",
    "result = spark.sql(sql)\n",
    "\n",
    "# 查看一条数据\n",
    "result = spark.sql(\"SELECT * FROM dwm_job LIMIT 1\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看时序数据总量\n",
    "result = spark.sql(\"SELECT count(*) FROM dwd_time_series\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT id,time,lon,lat\n",
    "FROM (\n",
    "\tSELECT id,time,lon, lat\n",
    "\tFROM dwd_time_series as time_series\n",
    "\tWHERE EXISTS (\n",
    "\t\tSELECT *\n",
    "\t\tFROM dwm_job as job\n",
    "\t\tWHERE job.id = 1\n",
    "\t\t\tAND time_series.time BETWEEN job.start_time AND job.end_time\n",
    "\t)\n",
    ") dwd_time_series;\n",
    "\n",
    "'''\n",
    "df_input = spark.sql(sql)\n",
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, TimestampType, FloatType, IntegerType\n",
    "\n",
    "# 定义模式（schema）\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"time\", TimestampType(), nullable=False),\n",
    "    StructField(\"lon\", FloatType(), nullable=False),\n",
    "    StructField(\"lat\", FloatType(), nullable=False)\n",
    "])\n",
    "\n",
    "# 添加分区列\n",
    "schema.add(StructField(\"area\", IntegerType(), nullable=False))\n",
    "\n",
    "# 打印模式（schema）\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "import rtree\n",
    "# 创建R树索引\n",
    "idx = rtree.index.Index()\n",
    "\n",
    "def build_rtree_idx(row):\n",
    "    idx.insert(row.id, (row.lon, row.lat, row.lon, row.lat))\n",
    "    \n",
    "def mapper(rows):\n",
    "    ResRow = Row()\n",
    "    # 针对一个part的数据做map\n",
    "    for row in rows:\n",
    "        \n",
    "        yield Row(id= row.id, time=row.time, lon=row.lon, lat=row.lat)\n",
    "        \n",
    "\n",
    "df_input.rdd.map(build_rtree_idx)\n",
    "\n",
    "# df_output = df_input.rdd.repartition(3000).mapPartitions(mapper).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MapOperation\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 创建一个包含数据的 PySpark DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "# 将 DataFrame 转换为 RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# 定义一个函数来应用于 RDD 的每个元素\n",
    "def map_func(rows):\n",
    "    \n",
    "    ret = []\n",
    "    ResRow = Row('Name','Age')\n",
    "    for row in rows:\n",
    "        name = row[\"Name\"]\n",
    "        age = row[\"Age\"]\n",
    "        \n",
    "        ret.append(ResRow(name,age))\n",
    "        \n",
    "#     return ret\n",
    "        \n",
    "\n",
    "# 应用 map 操作\n",
    "\n",
    "df_output = rdd.repartition(30).mapPartitions(map_func).toDF()\n",
    "\n",
    "# 将 RDD 转换回 DataFrame\n",
    "new_df = spark.createDataFrame(new_rdd, [\"Name\", \"Age\"])\n",
    "\n",
    "# 显示转换后的 DataFrame\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
